# Robust Detection Benchmark

This repository contains code, data and a benchmark leaderboard from the paper "Benchmarking Corruption Robustness in Object~Detection: Autonomous Driving when Winter is Coming" by Claudio Michaelis\*, Benjamin Mitzkus\*, Robert Geirhos\*, Evgenia Rusak\*, Oliver Bringmann, Matthias Bethge & Wieland Brendel.

The core idea is shown here: Real-world applications need to be able to cope with adverse outdoor hazards such as fog, frost, snow (and the occasional dragon fire). The paper benchmarks object detection models on their corruption resilience across a broad range of corruption types.

![traffic hazards](./figures/introduction/traffic_hazards.png)

## Structure & Overview

This repository serves two purposes:

1. Enabling reproducibility. All result figures from the directory ``figures/`` can be generated by executing the analysis notebook in ``data-analysis/`` which uses the data from ``raw-data/``.

2. Hosting the ``Robust Detection Benchmark`` (more information below).

Additionally, we provide three separate modules with functionality that we use in the paper and that we hope may be useful for your own research or applications. They are listed here:

Stylize arbitrary datasets:
https://github.com/bethgelab/stylize-datasets

Corrupt arbitrary datasets:
https://github.com/bethgelab/add-common-image-corruptions

Object detection:
https://github.com/bethgelab/robust_object_detection


## Robust Detection Benchmark

This section shows the most important results on our three benchmark datasets: 
COCO-C, Pascal-C and Cityscapes-C. All models have a fixed ResNet 50 backbone 
to put the focus on improvoemnts in detection robustness.
For more results including ones with different backbones and
instance segmentation results please have 
a look at the [Leaderboard](LEADERBOARD.md).

Results are ranked by their mean performance under corruption (named mCE in the paper). If you achieve state-of-the-art robustness on any of the three datasets with your approach, please open a pull request where you add the results in the table below. We strongly encourage to use backbone listed in the table below, otherwise robustness gains cannot be disentangled from improved overall performance. In your pull request, you will need to indicate the three metrics P, rPC and mPC (as defined in the paper); mPC will then be used to rank your results.

#### Pascal-C

Model  | Backbone  | box AP50 clean | box AP50 corr. | box % |
:-----:|:---------:|:--------------:|:--------------:|:-----:|
Faster R-CNN Combined | R-50-FPN | 80.4 | 56.2       | 69.9  |
Faster R-CNN | R-50-FPN          | 80.5 | 48.6       | 60.4  |


#### COCO-C

Model  | Backbone  | box AP clean | box AP corr. | box % |
:-----:|:---------:|:------------:|:------------:|:-----:| 
Faster R-CNN Combined | R-50-FPN | 34.6   | 20.4 | 58.9  |
Mask R-CNN   | R-50-FPN          | 37.3   | 18.7 | 50.1  |
Faster R-CNN | R-50-FPN          | 36.3   | 18.2 | 50.2  |
RetinaNet    | R-50-FPN          | 35.6   | 17.8 | 50.1  |

#### Cityscapes-C

Model  | Backbone  | box AP clean | box AP corr. | box % |
:-----:|:---------:|:------------:|:------------:|:-----:|
Faster R-CNN Combined | R-50-FPN | 36.3 | 17.2   | 47.4  |
Faster R-CNN | R-50-FPN  | 36.4   | 12.2         | 33.4  |
Mask R-CNN   | R-50-FPN  | 37.5   | 11.7         | 31.1  |

## Citation

If you use our code or the benchmark, please consider citing:
```
@article{xyz2019,
  title={xxx},
  author={xyz},
  journal={arXiv:},
  year={2019}
}
```
